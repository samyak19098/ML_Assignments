# -*- coding: utf-8 -*-
"""Q2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gCzBp9kkwP3wiMdJ1F7W17_XyhROZNcy
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn
from mlxtend.data import loadlocal_mnist
from sklearn.neural_network import MLPClassifier
from tqdm.notebook import tqdm
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import math
import pickle
import json

class MyNeuralNetwork():
    def __init__(self, n_layers, layer_sizes, activation, learning_rate, weight_init, batch_size, num_epochs):
        ''' Constructor for class, accepts 7 parameters as input as the class attributes
            n_layers: number of layers (including inut and output layer) (int), layer_sizes: a (n_layers size) list of sizes of layers of the Network, i.e- no. of nodes in each layer
            activation: activation function to be used (string), learning_rate: learning rate used during training(float),
            weight_init: initialzation function for weights (in {'normal', 'random', 'zero'}), batch_size: batch size to be used(int),
            num_epochs: number of epochs to be used(int), layer_parameters: Represents the parameters for each layer to be used. It is a dictionary of dictionary where each 
            inner dictionary holds value corresponding to keys "weights" and "bias". layer_parameters[i]['weights'] refers to a 2D numpy matrix of weights.
        '''
        
        self.n_layers = n_layers
        self.layer_sizes = layer_sizes
        self.activation = activation
        self.learning_rate = learning_rate
        self.weight_init = weight_init
        self.batch_size = batch_size
        self.num_epochs = num_epochs
        self.layer_parameters = self.init_params()
    
    def applyActivation(self, func, X):
        '''Activation function implemented over here.
            func: string representing what activation function to use
            X: The input numpy array on which activation function is to be applied
        '''
        if(func=="relu"):
            activation = np.maximum(0, X)
        elif(func == "leaky_relu"):
            activation = np.where(X > 0, X, 0.01 * X)
        elif(func == "sigmoid"):
            activation = 1 / (1 + np.exp(-1 * X))
        elif(func == "linear"):
            activation = X
        elif(func == "tanh"):
            activation = (np.exp(X) - np.exp(-1*X)) / (np.exp(X) + np.exp(-1*X))
        elif(func == "softmax"):
            num = np.exp(X)
            denom = np.sum(num, axis=1, keepdims=True)
            activation = num / denom
        
        return activation
    
    def calcActivationDeriv(self, func, X):
        ''' Gradients of the activation function implemented over here.
            func: string representing what activation function to calculate gradient for
            X: The input numpy array on which activation function's gradient is to be applied
        '''
        if(func=="relu"):
            derivative = np.where(X > 0, 1, 0)
        elif(func == "leaky_relu"):
            derivative = np.where(X > 0, 1, 0.01)
        elif(func == "sigmoid"):
            sigmoid_val = self.applyActivation(func, X)
            derivative = sigmoid_val * (1 - sigmoid_val)
        elif(func == "linear"):
            derivative = 1
        elif(func == "tanh"):
            tanh_val = self.applyActivation(func, X)
            derivative = 1 - np.square(tanh_val)
        elif(func == "softmax"):
            activation_val = self.applyActivation(func, X)
            derivative = activation_val * (1 - activation_val)

        return derivative

    def init_params(self):
        '''
            Initialize the model paramaters i.e - the layer-wise weights and the bias.
            Returns a dictionary of layer_parameters. Initializes the weights based on the initialization type. 
            The bias is initialize with zero.
        '''
        layer_parameters = {} #dictionary to hold parameters of the layer
        for layer_num in range(1, self.n_layers):
            layer_parameters[layer_num] = {}
            if(self.weight_init == "zero"):
                layer_parameters[layer_num]['weights'] = np.zeros((self.layer_sizes[layer_num], self.layer_sizes[layer_num - 1])) #zero init
            elif(self.weight_init == "random"):
                layer_parameters[layer_num]['weights'] = np.random.rand(self.layer_sizes[layer_num], self.layer_sizes[layer_num - 1]) * 0.01 #random init
            elif(self.weight_init == "normal"):
                layer_parameters[layer_num]['weights'] = np.random.normal(size=(self.layer_sizes[layer_num], self.layer_sizes[layer_num - 1])) * 0.01 #normal init
            layer_parameters[layer_num]['bias'] = np.zeros((1, self.layer_sizes[layer_num]))

        return layer_parameters

    def forward_propagation(self, X):
        '''
            Used to perform forward propagation in the neural network
            Input is a numpy array X which is forward propagated through the neural network.
            The final layer activations(output from the output layer), activations of all layers and the pre activations of all layers are returned.
            Pre activations refers to the induced local field and the activation is the value of activation fucntion applied on the same.
        '''
        pre_activations = {} #the dictionary hold preactivations for each layer
        activations = {} #the dictionary hold activations for each layer

        prev_activ = X
        pre_activations[0] = X
        activations[0] = X

        #iterating over all but last layer
        for layer_num in range(1, self.n_layers - 1):

            curr_pre_activ = np.dot(prev_activ, self.layer_parameters[layer_num]['weights'].T) + self.layer_parameters[layer_num]['bias']
            curr_activ = self.applyActivation(self.activation, curr_pre_activ)
            pre_activations[layer_num] = curr_pre_activ
            activations[layer_num] = curr_activ
            prev_activ = activations[layer_num]

        #last layer
        final_layer_pre_activ = np.dot(prev_activ, self.layer_parameters[self.n_layers - 1]['weights'].T) + self.layer_parameters[self.n_layers - 1]['bias']
        final_layer_activ = self.applyActivation("softmax", final_layer_pre_activ) #apply softmax at the final layer
        pre_activations[self.n_layers - 1] = final_layer_pre_activ
        activations[self.n_layers - 1] = final_layer_activ

        return final_layer_activ, pre_activations, activations

    def backward_propagation(self, X, Y, final_layer_activ, pre_activations, activations):
        ''' The function implements the backward propagation algorithm for neural networks.
            Input is numpy array X which refers to the array of inputs(like training input) and Y refers to the corresponding output labels.
            Other inputs are the activation i.e- the output of the final layer of the neural network, the preactivations of all the layers and the activations correspond to all the layers.
            The function performs one iteration of a backward propagation and updates the weights corresponding to each layer calculated during the bakcward propagation
        '''
        weight_updates = {} #used to store weight updates corresponding to each layer of the neural network
        bias_updates = {} #used to store bias updates corresponding to each layer of the neural network

        # output layer
        error_term_op = (final_layer_activ - Y)
        local_grad_op = error_term_op

        global_grad_op = np.dot(local_grad_op.T, activations[self.n_layers - 2])
        bias_updates[self.n_layers - 1] = np.sum(local_grad_op, axis=0, keepdims=True)
        weight_updates[self.n_layers - 1] = global_grad_op
        weighted_next_layer_grad =  np.dot(local_grad_op, self.layer_parameters[self.n_layers - 1]['weights'])

        #iterate bakcwards i.e starting from the second last layer to the first layer
        for layer_num in range(self.n_layers - 2, 0, -1):
            
            local_grad_layer = (self.calcActivationDeriv(self.activation, pre_activations[layer_num]) * weighted_next_layer_grad)
            global_grad_layer = np.dot(local_grad_layer.T, activations[layer_num - 1])
            bias_updates[layer_num] = np.sum(local_grad_layer, axis=0, keepdims=True) #store the bias update for the current layer
            weight_updates[layer_num] = global_grad_layer #store the weighted update for the current layer
            weighted_next_layer_grad = np.dot(local_grad_layer, self.layer_parameters[layer_num]['weights']) #recursive computation part i.e will be used in the next layer(the one to left of current layer)
        
        #update the weights and bias corresponding to each layer
        for layer_num in range(1, self.n_layers):
            self.layer_parameters[layer_num]['weights'] = self.layer_parameters[layer_num]['weights'] - self.learning_rate * (weight_updates[layer_num] / X.shape[0])
            self.layer_parameters[layer_num]['bias'] = self.layer_parameters[layer_num]['bias'] - self.learning_rate * (bias_updates[layer_num] / X.shape[0])
    
    def fit(self, x_train, y_train, x_val, y_val):
        '''
            It trains a new model on the passed training data
            Input : x_train: the inputs/feature data of the training set, y_train: the labels/outputs corresponding to samples in training set
            Also to calculate loss, we pass x_val, y_val: x_val: the inputs/feature data of the validation set, y_val: the labels/outputs corresponding to samples in validation set

        '''
        number_of_batches = int(x_train.shape[0] / self.batch_size)
        val_loss_list = []
        train_loss_list = []
        encoded_y_val = self.encode_classes(y_val)
        encoded_y_train = self.encode_classes(y_train)

        #iterate over all epochs
        for i in tqdm(range(self.num_epochs)):
            #iterate over all batches
            for batch_number in range(number_of_batches):
                start_idx = batch_number * self.batch_size
                if(batch_number == number_of_batches - 1):
                    end_idx = x_train.shape[0]
                else:
                    end_idx = (batch_number + 1) * self.batch_size
                x_train_batch = x_train[start_idx:end_idx, :]
                y_train_batch = self.encode_classes(y_train[start_idx:end_idx]) #encode the output labels in a binary format

                output_layer_activ, batch_pre_activations, batch_activations = self.forward_propagation(x_train_batch) #forward propagate on the training batch
                self.backward_propagation(x_train_batch, y_train_batch, output_layer_activ, batch_pre_activations, batch_activations) #backward propaget and update weights and bias using the current batch
            
            train_loss = self.calcLoss(self.predict_proba(x_train), encoded_y_train)
            val_loss = self.calcLoss(self.predict_proba(x_val), encoded_y_val)
            train_loss_list.append(train_loss)
            val_loss_list.append(val_loss)
            # print(f'epoch={i} : train_loss = {train_loss} || val_loss = {val_loss}')
            
        return train_loss_list, val_loss_list

    def predict_proba(self, input_data):
        ''' Used to get the class wise probability for the outputs corresponding to the passed input data
            Input: input_data: the input data passed
            Returns class-wise probability
        '''
        output_layer_activ, pre_activ, activ = self.forward_propagation(input_data)
        return output_layer_activ

    def predict(self, input_data):
        '''
            Used to find the predictions for the passed input data (for eg testing input data). 
            Returns the predictions array.
        '''
        output_layer_activ, pre_activ, activ = self.forward_propagation(input_data)
        predicitions = np.argmax(output_layer_activ, axis=1)
        return predicitions

    def score(self, input_data, labels):
        ''' Used to calculate the accuracy score.
            Input : input_data: the input data/ feature data, labels: the output labels corresponding to the input data
            Returns : the accuracy score
        '''
        predictions = self.predict(input_data)
        total = labels.shape[0]
        matches = np.sum(predictions == labels)
        score = matches / total

        return score

    def encode_classes(self, labels):
        '''
            Used to binary encode the labels inorder to be used in the neural network algorithms.
        '''
        labels = labels.reshape((1, labels.shape[0]))
        encoded_labels = np.zeros((labels.shape[1], self.layer_sizes[self.n_layers - 1]))
        for i in range(labels.shape[1]):
            encoded_labels[i, labels[0, i]] = 1
            
        return encoded_labels

    def calcLoss(self, predictions, Y):
        '''
            Used to calculate the cross entropy loss given the inputs : the predictions and the actual values(Y) {both array-like}
            returns : value of the loss
        '''
        log_probab = np.log(predictions + 1e-15)
        loss = -np.sum(log_probab * Y) / Y.shape[0]
        return loss

def split_test_train(X, Y, test_size):
    ''' Custom split function used to do train test splitting.
        Input: the input/feature data, Y: the output/label data
        test_size: ratio of test data size (scale 0 to 1)
        returns splitted data in form of four arrays.
    '''
    M = X.shape[0]
    num_train_sample = int((1 - test_size) * M)
    perm = np.random.permutation(M)
    x_shuffled = X[perm]
    y_shuffled = Y[perm]
    x_train = x_shuffled[:num_train_sample, :]
    x_test = x_shuffled[num_train_sample:, :]
    y_train = y_shuffled[:num_train_sample]
    y_test = y_shuffled[num_train_sample:]

    return x_train, x_test, y_train, y_test

def run_model_no_save(x_train, y_train, x_val, y_val, x_test, y_test, n_layers, layers_sizes, activation, learning_rate, weight_init, batch_size, epochs):

    my_nn = MyNeuralNetwork(n_layers, layers_sizes, activation, learning_rate, weight_init,  batch_size, epochs)
    train_losses, val_losses = my_nn.fit(x_train, y_train, x_val, y_val)

    acc = my_nn.score(x_test, y_test)
    acc_train = my_nn.score(x_train, y_train)

    print(f'train = {acc_train} || test = {acc}')
    plt.plot([i for i in range(epochs)], train_losses, label='train-loss', color='blue')
    plt.plot([i for i in range(epochs)], val_losses, label='val-loss', color='red')
    plt.legend()
    plt.xlabel('epochs')
    plt.ylabel('Loss')
    plt.title(f'Loss vs epochs: {activation}')
    plt.show()

    return my_nn, train_losses, val_losses, acc, acc_train

def run_model(x_train, y_train, x_val, y_val, x_test, y_test, n_layers, layers_sizes, activation, learning_rate, weight_init, batch_size, epochs):

    my_nn = MyNeuralNetwork(n_layers, layers_sizes, activation, learning_rate, weight_init,  batch_size, epochs)
    # file = open(f'./final/models/my_{activation}.pickle', 'wb')
    # pickle.dump(my_nn, file)
    # file.close()
    train_losses, val_losses = my_nn.fit(x_train, y_train, x_val, y_val)
    # np.save(f'./final/model_details/train_loss_{activation}.npy', np.array(train_losses))
    # np.save(f'./final/model_details/val_loss_{activation}.npy', np.array(val_losses))
    acc = my_nn.score(x_test, y_test)
    acc_train = my_nn.score(x_train, y_train)
    print(f'test accuracy = {acc}')
    plt.plot([i for i in range(epochs)], train_losses, label='train-loss', color='blue')
    plt.plot([i for i in range(epochs)], val_losses, label='val-loss', color='red')
    plt.legend()
    plt.xlabel('epochs')
    plt.ylabel('Loss')
    plt.title(f'Loss vs epochs: {activation}')
    # plt.savefig(f'./final/plots/my_{activation}.png', facecolor='w', bbox_inches='tight')
    plt.show()

    return my_nn, train_losses, val_losses, acc, acc_train

X_train, Y_train = loadlocal_mnist(images_path='./data/train-images.idx3-ubyte', labels_path='./data/train-labels.idx1-ubyte')
X_test, Y_test = loadlocal_mnist(images_path='./data/t10k-images.idx3-ubyte', labels_path='./data/t10k-labels.idx1-ubyte')

X_train = (X_train - X_train.mean())/ (X_train.std())
X_test = (X_test - X_test.mean())/ (X_test.std())

X_total = np.vstack((X_train, X_test))
Y_total = np.hstack((Y_train, Y_test))

X_train, X_comb, Y_train, Y_comb = split_test_train(X_total, Y_total, 0.3)
X_test, X_val, Y_test, Y_val = split_test_train(X_comb, Y_comb, 0.5)

relu_clf = MLPClassifier(hidden_layer_sizes=(256, 128, 64, 32), batch_size=100, solver='sgd', learning_rate='constant', learning_rate_init=0.08, activation='relu', max_iter=150, verbose=True)
relu_clf.fit(X_train, Y_train)
# pickle.dump(relu_clf, open('./final/models/relu_sk.sav', 'wb'))

relu_score_sk = relu_clf.score(X_test, Y_test)
print(relu_score_sk)

sigmoid_clf = MLPClassifier(hidden_layer_sizes=(256, 128, 64, 32), batch_size=100, solver='sgd', learning_rate='constant', learning_rate_init=0.08, activation='logistic', max_iter=150, verbose=True)
sigmoid_clf.fit(X_train, Y_train)
# pickle.dump(sigmoid_clf, open('./final/models/sigmoid_sk.sav', 'wb'))

sigmoid_score_sk = sigmoid_clf.score(X_test, Y_test)
print(sigmoid_score_sk)

linear_clf = MLPClassifier(hidden_layer_sizes=(256, 128, 64, 32), batch_size=100, solver='sgd', learning_rate='constant', learning_rate_init=0.08, activation='identity', max_iter=150, verbose=True)
linear_clf.fit(X_train, Y_train)
# pickle.dump(linear_clf, open('./final/models/linear_sk.sav', 'wb'))

linear_score_sk = linear_clf.score(X_test, Y_test)
print(linear_score_sk)

tanh_clf = MLPClassifier(hidden_layer_sizes=(256, 128, 64, 32), batch_size=100, solver='sgd', learning_rate='constant', learning_rate_init=0.08, activation='tanh', max_iter=150, verbose=True)
tanh_clf.fit(X_train, Y_train)
# pickle.dump(tanh_clf, open('./final/models/tanh_sk.sav', 'wb'))

tanh_score_sk = tanh_clf.score(X_test, Y_test)
print(tanh_score_sk)

num_features = X_train.shape[1]
num_classes = 10

"""My Models"""

final_nn_relu, train_loss_relu, val_loss_relu, acc_test_relu, acc_train_relu  = run_model(X_train, Y_train, X_val, Y_val, X_test, Y_test, 6, [num_features, 256, 128, 64, 32, num_classes], 'relu', 0.08, 'normal', 100, 150)

final_nn_sigmoid, train_loss_sigmoid, val_loss_sigmoid, acc_test_sigmoid, acc_train_sigmoid = run_model(X_train, Y_train, X_val, Y_val, X_test, Y_test, 6, [num_features, 256, 128, 64, 32, num_classes], 'sigmoid', 0.08, 'normal', 100, 150)

final_nn_softmax, train_loss_softmax, val_loss_softmax, acc_test_softmax, acc_train_softmax = run_model(X_train, Y_train, X_val, Y_val, X_test, Y_test, 6, [num_features, 256, 128, 64, 32, num_classes], 'softmax', 0.08, 'normal', 100, 150)

final_nn_linear, train_loss_linear, val_loss_linear, acc_test_linear, acc_train_linear = run_model(X_train, Y_train, X_val, Y_val, X_test, Y_test, 6, [num_features, 256, 128, 64, 32, num_classes], 'linear', 0.08, 'normal', 100, 150)

final_nn_tanh, train_loss_tanh, val_loss_tanh, acc_test_tanh, acc_train_tanh = run_model(X_train, Y_train, X_val, Y_val, X_test, Y_test, 6, [num_features, 256, 128, 64, 32, num_classes], 'tanh', 0.08, 'normal', 100, 150)

X_train_modif = X_train.copy()
X_train_modif = np.where(X_train_modif < 1e-9, 0, X_train_modif)
X_test_modif = X_test.copy()
X_test_modif = np.where(X_test_modif < 1e-9, 0, X_test_modif)
X_val_modif = X_val.copy()
X_val_modif = np.where(X_val_modif < 1e-9, 0, X_val_modif)

final_nn_leaky, train_loss_leaky, val_loss_leaky, acc_test_leaky, acc_train_leaky = run_model(X_train_modif, Y_train, X_val_modif, Y_val, X_test_modif, Y_test, 6, [num_features, 256, 128, 64, 32, num_classes], 'leaky_relu', 0.08, 'normal', 100, 150)

nn_relu1, train_loss_relu1, val_loss_relu1, acc_test_relu1, acc_train_relu1  = run_model_no_save(X_train, Y_train, X_val, Y_val, X_test, Y_test, 6, [num_features, 256, 128, 64, 32, num_classes], 'relu', 0.001, 'normal', 100, 150)

nn_relu1, train_loss_relu1, val_loss_relu1, acc_test_relu1, acc_train_relu1  = run_model_no_save(X_train, Y_train, X_val, Y_val, X_test, Y_test, 6, [num_features, 256, 128, 64, 32, num_classes], 'relu', 0.01, 'normal', 100, 120)

nn_relu1, train_loss_relu1, val_loss_relu1, acc_test_relu1, acc_train_relu1  = run_model_no_save(X_train, Y_train, X_val, Y_val, X_test, Y_test, 6, [num_features, 256, 128, 64, 32, num_classes], 'relu', 0.1, 'normal', 100, 150)

nn_relu1, train_loss_relu1, val_loss_relu1, acc_test_relu1, acc_train_relu1  = run_model_no_save(X_train, Y_train, X_val, Y_val, X_test, Y_test, 6, [num_features, 256, 128, 64, 32, num_classes], 'relu', 1, 'normal', 100, 150)